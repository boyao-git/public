
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am a Senior Lecturer (Associate Professor) at the Department of Psychology at Lancaster University, United Kingdom.\nMy research interests lie broadly within cognitive neuroscience of language. My current work includes neurocognitive mechanisms of inner speech and verbal hallucinations, abstract conceptual processing, discourse processing, bilingualism and self-awareness. I use s/fMRI, EEG, eye tracking, neurostimulation, and computational modelling in my research.\nMy work has been and/or is currently funded by the Economic and Social Research Council, Medical Research Council, The Bial Foundation, The British Academy and the Experimental Psychological Society.\n","date":1669852800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1669852800,"objectID":"ed0ccd63a82b4a29db8d2289861d99e5","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Senior Lecturer (Associate Professor) at the Department of Psychology at Lancaster University, United Kingdom.\nMy research interests lie broadly within cognitive neuroscience of language. My current work includes neurocognitive mechanisms of inner speech and verbal hallucinations, abstract conceptual processing, discourse processing, bilingualism and self-awareness. I use s/fMRI, EEG, eye tracking, neurostimulation, and computational modelling in my research.","tags":null,"title":"Bo Yao","type":"authors"},{"authors":null,"categories":null,"content":"Cognition is founded on the vast experiences we learn through our body. Embodied cognition refers to the idea that all conceptual knowledge has to be grounded in our bodily experiences, rather than arbitrary linguistic symbols.\nFor example, RED can only be meaningful if it is grounded in the visual experience of red-ness. When we hear or read this word, our brain would re-activate, or ‘mentally simulate’, some of that learned visual experience for us to understand what RED means. In comparison, RED would not be represented in such ways for people who are born blind.\nWe are very interested in how abstract concepts like LOVE or TRUST are grounded in bodily experiences. Although they do not have direct referents in the physical world, new research suggests that abstract meaning could be supported by experiences inside the body in the emotional, introspective, and interoceptive (internal organs) domains. For example, our recent work asks how abstract concepts like TIME, IDEA, MISTAKE that are invisible to the eye can have size (e.g., “Tomorrow is my big day”, “I like big ideas”, “I made a small mistake”)? It turns out that abstract size may be derived from how emotionally evoking the word is and from metaphorical associations with concrete words that do have size (e.g., “The responsibility is like a mountain on my shoulders”). In other words, abstract concepts could use emotional experiences, or visual experiences of their concrete counterparts to represent its size.\nUnderstanding conceptual processing is a challenge but also good fun. There are many questions that fascinate us, for example:\nHow are embodied representations of language flexibly activated across contexts and task demands?\nWhat are the neurocomputational mechanisms of embodied mental simulations?\nHow do people who lack mental imagery understand concepts?\nWhat are the relative contributions of linguistic and embodied experiences in conceptual processing? How do they change across contexts and lifespan?\nIf you are interested in this area, and would like to work with us, please do get in touch.\n-–\nBack to Research\nNext Topic (Psycholinguistics)\n-–\nExample papers from the lab: Bo Yao, Jack E. Taylor, Sara C. Sereno (2022). What can size tell us about abstract conceptual processing?. Journal of Memory and Language, 127, 104369. PDF DOI Bo Yao (2021). Mental simulations of phonological representations are causally linked to silent reading of direct versus indirect speech. Journal of Cognition, 4(1), 6. PDF DOI Bo Yao, Anne Keitel, Gillian Bruce, Graham G. Scott, Patrick J. O\u0026#39;Donnell, Sara C. Sereno (2018). Differential emotional processing in concrete and abstract words. Journal of Experimental Psychology: Learning, Memory and Cognition, 44(7), 1064-1074. PDF DOI Bo Yao, Milica Vasiljevic, Mario Weick, Margaret E. Sereno, Patrick J. O\u0026#39;Donnell, Sara C. Sereno (2013). Semantic size of abstract concepts: It gets emotional when you can’t see it. PLoS ONE, 8(9), e75000. PDF DOI ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fd1815e59d374a5dd4bb386f34fba7ca","permalink":"https://boyao.science/research/embodied-cognition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/embodied-cognition/","section":"research","summary":"We are all born a blank canvas. We learn about the world and ourselves through our body (e.g,. the senses, affect, interoception, introception). These embodied experiences are the foundation of who we are.","tags":["embodied cognition"],"title":"Embodied Cognition","type":"research"},{"authors":null,"categories":null,"content":"Our rich, embodied experiences are compressed, structured and extended by a symbolic system called language so we can think and communicate efficiently and creatively.\nFor example, if I want to talk about an iconic Italian food, I do not have to mime, or draw a round-shaped bread with toppings every single time. I just say ‘pizza’, and you know what it is. It is very efficient.\nWe are very interested in how sentence prosody (i.e. intonation, rhythm) interacts with syntactic processing and how structural representations are implemented in neurocognitive systems. We also work on attention models of eye movements in reading, how reading direction of a language (e.g,. English vs. Hebrew) affects the spatial representation of sentence meaning, and how subject-prominent languages (e.g., English) vs. topic-prominent languages (e.g., Mandarin Chinese) may be represented in different structures.\nPsycholinguistics is a hard science and great fun, according to psycholinguists. People have been playing with languages for decades yet there are still many questions that baffle us to this day, for instance:\nIs attention allocated to single words serially or to multiple words in parallel in reading (debate is still ongoing)?\nIs syntactic structure ’embodied’ in prosody (a sensory experience)?\nDoes syntactic flexibility (i.e. some languages have flexible word order) reflect flexibility in the structural representation?\nHow is hierarchical structure (e.g., sentences with sub-clauses) represented neurocomputationally?\nIf you are a geek who is into good old psycholinguistics, and want to work together, please do get in touch :)\n-–\nLast Topic (Embodied Cognition)\nNext Topic (Inner Speech)\n-–\nExample Papers Graham G. Scott, Anne Keitel, Marc Becirspahic, Bo Yao, Sara C. Sereno (2019). The Glasgow Norms: Ratings of 5,500 words on nine scales. Behavior Research Methods, 51(3), 1258-1270. PDF DOI Sara C. Sereno, Christopher J. Hand, Aisha Shahid, Bo Yao, Patrick J. O\u0026#39;Donnell (2018). Testing the limits of contextual constraint: Interactions with word frequency and parafoveal preview during fluent reading. Quarterly Journal of Experimental Psychology, 71(1), 302-313. PDF DOI Bo Yao, Anne Keitel, Gillian Bruce, Graham G. Scott, Patrick J. O\u0026#39;Donnell, Sara C. Sereno (2018). Differential emotional processing in concrete and abstract words. Journal of Experimental Psychology: Learning, Memory and Cognition, 44(7), 1064-1074. PDF DOI Sara C. Sereno, Graham G. Scott, Bo Yao, Elske J. Thaden, Patrick J. O\u0026#39;Donnell (2015). Emotion word processing: Does mood make a difference?. Frontiers in Psychology, 6:1191. PDF DOI ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"869df958230571f1ca781e262123a397","permalink":"https://boyao.science/research/psycholinguistics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/psycholinguistics/","section":"research","summary":"Embodied experiences are compressed, structured and extended by language. How does language work as a symbolic system and how does it interface with psychological processes?","tags":["psycholinguistics","syntactic priming","prosody","pragmatics"],"title":"Psycholinguistics","type":"research"},{"authors":null,"categories":null,"content":"Inner speech, the experience of talking to oneself in the mind, is one of the most commonly experienced yet least scientifically understood mental activities.\nFor example, when you mind wanders off in a lecture, you may hear yourself say: “Hmm… What shall I have for lunch?” or “Gosh! This lecture is really boring!”. When you try to remember a phone number, you rehearse the digits “071234…” in your mind until you write it down.\nIn a nutshell, I’d like to think inner speech as conscious expressions of linguistically structured thoughts. Research on inner speech is quite limited, in both scope and methods. The reality is that inner speech takes many different forms and is likely supported by multiple mechanisms. In our lab, we are particularly interested in how inner speech’s diverse phenomenology (e.g., dialogic/monologic, intentional/spontaneous) is supported by neurocognitive mechanisms of the brain. Our current model involves a dual-stream mechanism where the phenomenological variety of inner speech are jointly supported by a mechanism based on corollary discharge and a mechanism based on perceptual simulation (re-activation of sensory experience). We use a combination of fMRI, EEG to characterise the neurophysiological signatures of inner speech in various task conditions, and use neurostimulation to examine its functional roles in cognition and consciousness, within and between individuals.\nStudying inner speech is very interesting because it is a very intimate experience shared by most of us. It is also very challenging because it is too private to be directly observed by researchers. There are some crazy questions you can ask about inner speech, e.g.:\nDo deaf people hear inner speech? (Apparantly they say they do - even for congenitally deaf individuals)\nIf they do, what is the nature / representation of their inner speech (perhaps inner speech is not so much about the sound)?\nDoes sutterers’ inner speech stutter?\nWhy do we need inner speech? Why is it there? (hard question! and apparently some people don’t have inner speech - you may be surprised - and how does their brain work then?)\nThe list is endless…\nSo if this has not piqued your interest, I don’t know what will. Please get in touch and let’s do something interesting :)\n-–\nLast Topic (Psycholinguistics)\nNext Topic (Consciousness)\n-–\nExample Papers: Bo Yao, Jason R. Taylor, Briony Banks, Sonja A. Kotz (2021). Reading direct speech quotes increases theta phase-locking: Evidence for cortical tracking of inner speech?. NeuroImage, 239, 118313. PDF DOI Ben Alderson-Day, Jamie Moffatt, Marco Bernini, Kaja Mitrenga, Bo Yao, Charles Fernyhough (2020). Processing speech and thoughts during silent reading: Direct reference effects for speech by fictional characters in voice-selective auditory cortex and a Theory-of-Mind network. Journal of Cognitive Neuroscience, 32(9), 1637-1653. PDF DOI Bo Yao, Christoph Scheepers (2015). Inner voice experiences during processing of direct and indirect speech. in Frazier, L., \u0026amp; Gibson, E. (Ed.), Explicit and Implicit Prosody in Sentence Processing (pp. 287-307). Springer.. PDF DOI Bo Yao, Christoph Scheepers (2011). Contextual modulation of reading rate for direct versus indirect speech quotations. Cognition, 121(3), 447-453. PDF DOI Bo Yao, Pascal Belin, Christoph Scheepers (2011). Silent reading of direct versus indirect speech activates voice-selective areas in the auditory cortex. Journal of Cognitive Neuroscience, 23(10), 3146-3152. PDF DOI ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"aa5587671b7542bfd054f11ae7becc58","permalink":"https://boyao.science/research/inner-speech/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/inner-speech/","section":"research","summary":"A sensorimotor manifestation of language and thoughts. Why do we generate and perceive inner speech? Is it instrumental to cognition and consciousness, or is it a by-product of thinking?","tags":["inner speech","neural oscillations","corollary discharge","perceptual simulation"],"title":"Inner Speech","type":"research"},{"authors":null,"categories":null,"content":"Ahhh consciousness, the most baffling mystery in the science of the mind. There is nothing that we know more intimately than conscious experience, but there is nothing that is harder to explain.\nIndeed, why do we experience consciousness? Why hasn’t evolution produced a bunch of ‘intelligent’ robots without it? Philosophers argue that human consciousness is underpinned by our unique abilities to become self-aware and to develop a concept of the self (e.g., with identities, values and goals) and many believe that self-awareness is generated by language, which is often expressed in inner speech (e.g., “I am angry”).\nConsciousness is a broad subject, so we try to understand it in the special case of auditory verbal hallucinations (AVHs). AVHs are ‘distorted’ conscious experiences of hearing voices that don’t exist in the outside world. Evidently these voices must be generated by the brain but aren’t recognised as coming from ’the self’. How so?\nTo understand this better, we build paradigms that can experimentally induce hallucination-like responses, such as detecting a spoken word in noise when no word is spoken. We tweak different parameters of the paradigms and see what factors can increase or decrease false perception of speech and explore how they are implemented neurocomputationally. By doing so, we can test to what extend our conscious reality is driven by expectation (what the brain wants to experience) and how much is supported by perception (what the brain receives from the senses).\nAs usual, many questions remain unanswered at this point of history:\nTo what extend are verbal halluciantions mis-attribuetd inner speech vs. intrusive memories of speech vs. mis-interpretation speech perception?\nDo hallucinators experience the self differently from non-hallucinators?\nDo people with language disorders (e.g., dyslexia, aphasia) experience the self differently?\nCan you generate self-awareness in artificial intelligence by giving it a form of natural language?\nIf consciousness is something that tickles your fancy, please do get in touch for a chat :)\n-–\nLast Topic (Inner Speech)\nBack to Research\n-–\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fbd8d06503296b5cd6748a2a96c37ac0","permalink":"https://boyao.science/research/consciousness/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/consciousness/","section":"research","summary":"Many animals appear to have conscious awareness of their physical but only humans appear to have a sense of the self. Why do we need self-awareness? How do we 'know' ourself? What is language's role in our subjective existence?","tags":["consciousness","self-awareness","auditory verbal hallucinations"],"title":"Consciousness","type":"research"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://boyao.science/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Bo Yao","Jack E. Taylor","Sara C. Sereno"],"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"e717350af3f3e91d7469ef8d8c38d483","permalink":"https://boyao.science/publication/2022-yts-jml/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/2022-yts-jml/","section":"publication","summary":"Embodied cognition theories propose that abstract concepts are grounded in a variety of exogenous and endogenous experiences which may be flexibly activated across contexts and tasks. In three experiments, we explored how semantic size (i.e., the magnitude, dimension or extent of an object or a concept) of abstract (vs concrete) concepts is mentally represented. We show that abstract size is metaphorically associated with the physical size of concrete objects (Experiment 1) and can produce a semantic-font size congruency effect comparable to that demonstrated in concrete words during online lexical processing (Experiment 2). Critically, this size congruency effect is large when a word is judged by its semantic size but significantly smaller when it is judged by its emotionality (Experiment 3), regardless of concreteness. Our results suggest that semantic size of abstract concepts can be grounded in visual size, which is activated adaptively under different task demands. The present findings advocate flexible embodiment of semantic representations, with an emphasis on the role of task effects on conceptual processing.","tags":["abstract concepts","semantic size","embodied cognition","emotion","metaphor"],"title":"What can size tell us about abstract conceptual processing?","type":"publication"},{"authors":null,"categories":null,"content":"Welcome to EPIC lab!\nWe are based at Lancaster University, United Kingdom.\nIt has a vibrant and beautiful campus.\nPeople Bo Yao Facilities A fleet of ","date":1666915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666915200,"objectID":"83e6d9c6011041b24c7f25baa6d37901","permalink":"https://boyao.science/epic-lab/","publishdate":"2022-10-28T00:00:00Z","relpermalink":"/epic-lab/","section":"","summary":"Welcome to EPIC lab!\nWe are based at Lancaster University, United Kingdom.\nIt has a vibrant and beautiful campus.\nPeople Bo Yao Facilities A fleet of ","tags":null,"title":"Join the Lab","type":"page"},{"authors":null,"categories":null,"content":"I am privacy-conscious, and I am against any form of non-essential personal data collection.\nThis site is a static site (it is just like a bunch of posters on show). It has no scripts and does not collect any information from you.\nNote: if you email me, you inevitably reveal your email address and your name (not necessarily your real name), which are necessary for communication but are not considered sensitive information.\n---\nA Little Segway If you are interested in preserving a bit of privacy where you can, you could check out https://www.privacytools.io/ for a list of privacy-focused apps and services than you could try. In principle, use end-to-end-encryption (E2EE) apps and services wherever you can, block tracking when browsing the web and limit apps’ access to your phone’s sensors as much as possible (or turn the sensors off completely).\nHave fun and enjoy the internet the way it’s meant to be.\n","date":1665792000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665792000,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://boyao.science/privacy/","publishdate":"2022-10-15T00:00:00Z","relpermalink":"/privacy/","section":"","summary":"I am privacy-conscious, and I am against any form of non-essential personal data collection.\nThis site is a static site (it is just like a bunch of posters on show). It has no scripts and does not collect any information from you.\nNote: if you email me, you inevitably reveal your email address and your name (not necessarily your real name), which are necessary for communication but are not considered sensitive information.","tags":null,"title":"Privacy Policy","type":"page"},{"authors":["Bo Yao","Jason R. Taylor","Briony Banks","Sonja A. Kotz"],"categories":null,"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"60bcc8fd808fc22a64c9a0898f940ade","permalink":"https://boyao.science/publication/2021-ytbk-neuroimage/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/2021-ytbk-neuroimage/","section":"publication","summary":"Growing evidence shows that theta-band (4–7 Hz) activity in the auditory cortex phase-locks to rhythms of overt speech. Does theta activity also encode the rhythmic dynamics of inner speech? Previous research established that silent reading of direct speech quotes (e.g., _Mary said: “This dress is lovely!”_) elicits more vivid inner speech than indirect speech quotes (e.g., _Mary said that the dress was lovely_). As we cannot directly track the phase alignment between theta activity and inner speech over time, we used EEG to measure the brain's phase-locked responses to the onset of speech quote reading. We found that direct (vs. indirect) quote reading was associated with increased theta phase synchrony over trials at 250–500 ms post-reading onset, with sources of the evoked activity estimated in the speech processing network. An eye-tracking control experiment confirmed that increased theta phase synchrony in direct quote reading was not driven by eye movement patterns, and more likely reflects synchronous phase resetting at the onset of inner speech. These findings suggest a functional role of theta phase modulation in reading-induced inner speech.","tags":["inner speech","neural oscillations","phase synchrony","phase-locking","silent reading","theta oscillations","EEG","eye tracking"],"title":"Reading direct speech quotes increases theta phase-locking: Evidence for cortical tracking of inner speech?","type":"publication"},{"authors":["Bo Yao"],"categories":null,"content":"","date":1610064000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610064000,"objectID":"5f58312758838b5b55f52d02eca5ce07","permalink":"https://boyao.science/publication/2021-y-jocognition/","publishdate":"2021-01-08T00:00:00Z","relpermalink":"/publication/2021-y-jocognition/","section":"publication","summary":"Embodied theories propose that language is understood via mental simulations of sensory states related to perception and action. Given that direct speech (e.g., _She says, “It’s a lovely day!”_) is perceived to be more vivid than indirect speech (e.g., _She says (that) it’s a lovely day_) in perception, recent research shows in silent reading that more vivid speech representations are mentally simulated for direct speech than for indirect speech. This ‘simulated’ speech is found to contain suprasegmental prosodic representations (e.g., speech prosody) but its phonological detail and its causal role in silent reading of direct speech remain unclear. Here in three experiments, I explored the phonological aspect and the causal role of speech simulations in silent reading of tongue twisters in direct speech, indirect speech and non-speech sentences. The results demonstrated greater visual tongue-twister effects (phonemic interference) during silent reading (Experiment 1) but not oral reading (Experiment 2) of direct speech as compared to indirect speech and non-speech. The tongue-twister effects in silent reading of direct speech were selectively disrupted by phonological interference (concurrent articulation) as compared to manual interference (finger tapping) (Experiment 3). The results replicated more vivid speech simulations in silent reading of direct speech, and additionally extended them to the phonological dimension. Crucially, they demonstrated a causal role of phonological simulations in silent reading of direct speech, at least in tongue-twister reading. The findings are discussed in relation to multidimensionality and task dependence of mental simulation and its mechanisms.","tags":["inner speech","tongue twisters","phonemic interference","eye tracking","silent reading","speech quotations","embodied cognition","articulatory suppression"],"title":"Mental simulations of phonological representations are causally linked to silent reading of direct versus indirect speech","type":"publication"},{"authors":["admin","吳恩達"],"categories":["Demo","教程"],"content":"Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy’s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://boyao.science/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Ben Alderson-Day","Jamie Moffatt","Marco Bernini","Kaja Mitrenga","Bo Yao","Charles Fernyhough"],"categories":null,"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"b1768e92e360696878ac5fa3324db7d7","permalink":"https://boyao.science/publication/2020-ambmyf-jocn/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/publication/2020-ambmyf-jocn/","section":"publication","summary":"Stories transport readers into vivid imaginative worlds, but understanding how readers create such worlds—populating them with characters, objects, and events—presents serious challenges across disciplines. Auditory imagery is thought to play a prominent role in this process, especially when representing characters' voices. Previous research has shown that direct reference to speech in stories (e.g., _He said, “I'm over here”_) may prompt spontaneous activation of voice-selective auditory cortex more than indirect speech [Yao, B., Belin, P., \u0026 Scheepers, C. Silent reading of direct versus indirect speech activates voice-selective areas in the auditory cortex. Journal of Cognitive Neuroscience, 23, 3146–3152, 2011]. However, it is unclear whether this effect reflects differential processing of speech or differences in linguistic content, source memory, or grammar. One way to test this is to compare direct reference effects for characters speaking and thinking in a story. Here, we present a multidisciplinary fMRI study of 21 readers' responses to characters' speech and thoughts during silent reading of short fictional stories. Activations relating to direct and indirect references were compared for both speaking and thinking. Eye-tracking and independent localizer tasks (auditory cortex and theory of mind [ToM]) established ROIs in which responses to stories could be tracked for individuals. Evidence of elevated auditory cortex responses to direct speech over indirect speech was observed, replicating previously reported effects; no reference effect was observed for thoughts. Moreover, a direct reference effect specific to speech was also evident in regions previously associated with inferring intentions from communication. Implications are discussed for the spontaneous representation of fictional characters and the potential roles of inner speech and ToM in this process.","tags":["inner speech","fMRI","eye tracking","speech quotations","thoughts","silent reading","Theory of Mind"],"title":"Processing speech and thoughts during silent reading: Direct reference effects for speech by fictional characters in voice-selective auditory cortex and a Theory-of-Mind network","type":"publication"},{"authors":["Graham G. Scott","Anne Keitel","Marc Becirspahic","Bo Yao","Sara C. Sereno"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"c7dfeadbd34de06bbc2f4abfdbbe9c08","permalink":"https://boyao.science/publication/2019-skbys-brm/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/2019-skbys-brm/","section":"publication","summary":"The Glasgow Norms are a set of normative ratings for 5,553 English words on nine psycholinguistic dimensions: arousal, valence, dominance, concreteness, imageability, familiarity, age of acquisition, semantic size, and gender association. The Glasgow Norms are unique in several respects. First, the corpus itself is relatively large, while simultaneously providing norms across a substantial number of lexical dimensions. Second, for any given subset of words, the same participants provided ratings across all nine dimensions (33 participants/word, on average). Third, two novel dimensions—semantic size and gender association—are included. Finally, the corpus contains a set of 379 ambiguous words that are presented either alone (e.g., toast) or with information that selects an alternative sense (e.g., toast (bread), toast (speech)). The relationships between the dimensions of the Glasgow Norms were initially investigated by assessing their correlations. In addition, a principal component analysis revealed four main factors, accounting for 82% of the variance (Visualization, Emotion, Salience, and Exposure). The validity of the Glasgow Norms was established via comparisons of our ratings to 18 different sets of current psycholinguistic norms. The dimension of size was tested with megastudy data, confirming findings from past studies that have explicitly examined this variable. Alternative senses of ambiguous words (i.e., disambiguated forms), when discordant on a given dimension, seemingly led to appropriately distinct ratings. Informal comparisons between the ratings of ambiguous words and of their alternative senses showed different patterns that likely depended on several factors (the number of senses, their relative strengths, and the rating scales themselves). Overall, the Glasgow Norms provide a valuable resource—in particular, for researchers investigating the role of word recognition in language comprehension.","tags":["psycholinguistic norms","arousal","valence","dominance","concreteness","imageability","familiarity","age of acquisition","semantic size","gender association"],"title":"The Glasgow Norms: Ratings of 5,500 words on nine scales","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://boyao.science/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Bo Yao","Christoph Scheepers"],"categories":null,"content":"","date":1531612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531612800,"objectID":"2f391990e9f0678304dcd8035c72b15f","permalink":"https://boyao.science/publication/2018-ys-cognition/","publishdate":"2018-07-15T00:00:00Z","relpermalink":"/publication/2018-ys-cognition/","section":"publication","summary":"The implicit prosody hypothesis (Fodor, 1998, 2002) proposes that silent reading coincides with a default, implicit form of prosody to facilitate sentence processing. Recent research demonstrated that a more vivid form of implicit prosody is mentally simulated during silent reading of direct speech quotations (e.g., _Mary said, “This dress is beautiful”_), with neural and behavioural consequences (e.g., Yao, Belin, \u0026 Scheepers, 2011; Yao \u0026 Scheepers, 2011). Here, we explored the relation between ‘default’ and ‘simulated’ implicit prosody in the context of relative-clause (RC) attachment in English. Apart from conﬁrming a general low RC-attachment preference in both production (Experiment 1) and comprehension (Experiments 2 and 3), we found that during written sentence completion (Experiment 1) or when reading silently (Experiment 2), the low RC-attachment preference was reliably enhanced when the critical sentences were embedded in direct speech quotations as compared to indirect speech or narrative sentences. However, when reading aloud (Experiment 3), direct speech did not enhance the general low RC-attachment preference. The results from Experiments 1 and 2 suggest a quantitative boost to implicit prosody (via auditory perceptual simulation) during silent production/comprehension of direct speech. By contrast, when reading aloud (Experiment 3), prosody becomes equally salient across conditions due to its explicit nature; indirect speech and narrative sentences thus become as susceptible to prosody-induced syntactic biases as direct speech. The present ﬁndings suggest a shared cognitive basis between default implicit prosody and simulated implicit prosody, providing a new platform for studying the eﬀects of implicit prosody on sentence processing.","tags":["inner speech","implicit prosody","relative-clause attachment","speech quotations","mental simulation"],"title":"Direct speech quotations promote low relative-clause attachment in silent reading of English","type":"publication"},{"authors":["Andrew J. Stewart","Jeffrey S. Wood","Elizabeth Le-Luan","Bo Yao","Matthew Haigh"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"9095462d1d44700c44b3f697b9776122","permalink":"https://boyao.science/publication/2018-swlyh-qjep/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/2018-swlyh-qjep/","section":"publication","summary":"In an eye-tracking experiment, we examined how readers comprehend indirect replies when they are uttered in reply to a direct question. Participants read vignettes that described two characters engaged in dialogue. Each dialogue contained a direct question (e.g., How are you doing in Chemistry?) answered with an excuse (e.g., The exams are not fair). In response to direct questions, such indirect replies are typically used to avoid a face-threatening disclosure (e.g., doing badly on the Chemistry course). Our goal was to determine whether readers are sensitive during reading to the indirect meaning communicated by such replies. Of the three contexts we examined, the first described a negative, face-threatening situation and the second a positive, non-face threatening situation, while the third was neutral. Analysis of reading times to the replies provides strong evidence that readers are sensitive online to the face-saving function of indirect replies.","tags":["face management","indirect meaning","indirect replies","discourse processing"],"title":"‘It’s hard to write a good article’: The online comprehension of excuses as indirect replies","type":"publication"},{"authors":["Andrew J. Stewart","Elizabeth Le-Luan","Jeffrey S. Wood","Bo Yao","Matthew Haigh"],"categories":null,"content":"","date":1518825600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518825600,"objectID":"06d78b439a6ee1d26dbadf8fc7d87dbc","permalink":"https://boyao.science/publication/2018-slwyh-discourseprocesses/","publishdate":"2018-02-17T00:00:00Z","relpermalink":"/publication/2018-slwyh-discourseprocesses/","section":"publication","summary":"In everyday conversation much communication is achieved using indirect language. This is particularly true when we utter requests. The decision to use indirect language is influenced by a number of factors, including deniability, politeness, and the degree of imposition on the receiver of a request. In this article we report the results of an eye-tracking experiment examining the influence on reading of the degree of imposition of a request. We manipulate whether context describes a situation in which the level of imposition on the receiver of the request is high (which thus motivates the use of indirect language) with one in which the level of imposition is low (and thus does not motivate the use of indirect language). We compare the comprehension of statements that are phrased indirectly with the comprehension of statements that are phrased more directly. We find that statements phrased indirectly are read more quickly in contexts where the level of imposition on the receiver is high versus when the level of imposition is low. In contrast, we find the processing of statements phrased directly does not vary as a function of level of imposition. This indicates that readers use pragmatic knowledge to guide interpretation of indirect requests. Our data provide an insight into the interface between pragmatic and semantic processing.","tags":["pragmatics","indirect requests","communication","discourse processing"],"title":"Comprehension of indirect requests is influenced by their degree of imposition","type":"publication"},{"authors":["Bo Yao","Anne Keitel","Gillian Bruce","Graham G. Scott","Patrick J. O'Donnell","Sara C. Sereno"],"categories":null,"content":"","date":1518393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518393600,"objectID":"e43947d0f686c721d3e0d51b5603ed68","permalink":"https://boyao.science/publication/2018-ykbsos-jeplmc/","publishdate":"2018-02-12T00:00:00Z","relpermalink":"/publication/2018-ykbsos-jeplmc/","section":"publication","summary":"Emotion (positive and negative) words are typically recognized faster than neutral words. Recent research suggests that emotional valence, while often treated as a unitary semantic property, may be differentially represented in concrete and abstract words. Studies that have explicitly examined the interaction of emotion and concreteness, however, have demonstrated inconsistent patterns of results. Moreover, these findings may be limited as certain key lexical variables (e.g., familiarity, age of acquisition) were not taken into account. We investigated the emotion-concreteness interaction in a large-scale, highly controlled lexical decision experiment. A 3 (Emotion: negative, neutral, positive) × 2 (Concreteness: abstract, concrete) design was used, with 45 items per condition and 127 participants. We found a significant interaction between emotion and concreteness. Although positive and negative valenced words were recognized faster than neutral words, this emotion advantage was significantly larger in concrete than in abstract words. We explored potential contributions of participant alexithymia level and item imageability to this interactive pattern. We found that only word imageability significantly modulated the emotion-concreteness interaction. While both concrete and abstract emotion words are advantageously processed relative to comparable neutral words, the mechanisms of this facilitation are paradoxically more dependent on imageability in abstract words.","tags":["emotion","concreteness","embodied cognition","alexithymia","imageability"],"title":"Differential emotional processing in concrete and abstract words","type":"publication"},{"authors":["Sara C. Sereno","Christopher J. Hand","Aisha Shahid","Bo Yao","Patrick J. O'Donnell"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"22141f8672af8f64d96870e0ea1dbcd9","permalink":"https://boyao.science/publication/2018-shsyo-qjep/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/2018-shsyo-qjep/","section":"publication","summary":"Contextual constraint is a key factor affecting a word’s fixation duration and its likelihood of being fixated during reading. Previous research has generally demonstrated additive effects of predictability and frequency in fixation times. Studies examining the role of parafoveal preview have shown that greater preview benefit is obtained from more predictable and higher frequency words versus less predictable and lower frequency words. In two experiments, we investigated effects of target word predictability, frequency and parafoveal preview. A 3 (Predictability: low, medium, high) × 2 (Frequency: low, high) design was used with Preview (valid, invalid) manipulated between experiments. With valid previews, we found main effects of Predictability and Frequency in both fixation time and fixation probability measures, including an interaction in early fixation measures. With invalid preview, we again found main effects of Predictability and Frequency in fixation times, but no evidence of an interaction. Fixation probability showed a weak Predictability effect and Predictability–Frequency interaction. Predictability interacted with Preview in early fixation time and fixation probability measures. Our findings suggest that high levels of contextual constraint exert an early influence during lexical processing in reading. Results are discussed in terms of models of language processing and eye movement control.","tags":["contextual predictability","word frequency","parafoveal preview","eye tracking","silent reading"],"title":"Testing the limits of contextual constraint: Interactions with word frequency and parafoveal preview during fluent reading","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://boyao.science/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":["Mario Weick","John A. Allen","Milica Vasiljevic","Bo Yao"],"categories":null,"content":"","date":1454284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454284800,"objectID":"c67a4bea849bd72e344ddd7afb4305a2","permalink":"https://boyao.science/publication/2016-wavy-cognition/","publishdate":"2016-02-01T00:00:00Z","relpermalink":"/publication/2016-wavy-cognition/","section":"publication","summary":"Healthy individuals display a tendency to allocate attention unequally across space, and this bias has implications for how individuals interact with their environments. However, the origins of this phenomenon remain relatively poorly understood. The present research examined the joint and independent contributions of two fundamental motivational systems – behavioural approach and inhibition systems (BAS and BIS) – to lateral spatial bias in a locomotion task. Participants completed self-report measures of trait BAS and BIS, then repeatedly traversed a room, blindfolded, aiming for a straight line. We obtained locomotion data from motion tracking to capture variations in the walking trajectories. Overall, walking trajectories deviated to the left, and this tendency was more pronounced with increasing BIS scores. Meanwhile, BAS was associated with relative rightward tendencies when BIS was low, but not when BIS was high. These results demonstrate for the first time an association between BIS and lateral spatial bias independently of variations in BAS. The findings also contribute to clarify the circumstances in which BAS is associated with a rightward bias. We discuss the implications of these findings for the neurobiological underpinnings of BIS and for the literature on spatial bias.","tags":["spatial bias","lateralisation","motion tracking"],"title":"Walking blindfolded unveils unique contributions of behavioural approach and inhibition to lateral spatial bias","type":"publication"},{"authors":["Sara C. Sereno","Graham G. Scott","Bo Yao","Elske J. Thaden","Patrick J. O'Donnell"],"categories":null,"content":"","date":1440374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440374400,"objectID":"0db14ba45d7b086a696c17eaafeac461","permalink":"https://boyao.science/publication/2015-ssyto-frontierpsych/","publishdate":"2015-08-24T00:00:00Z","relpermalink":"/publication/2015-ssyto-frontierpsych/","section":"publication","summary":"Visual emotion word processing has been in the focus of recent psycholinguistic research. In general, emotion words provoke differential responses in comparison to neutral words. However, words are typically processed within a context rather than in isolation. For instance, how does one's inner emotional state influence the comprehension of emotion words? To address this question, the current study examined lexical decision responses to emotionally positive, negative, and neutral words as a function of induced mood as well as their word frequency. Mood was manipulated by exposing participants to different types of music. Participants were randomly assigned to one of three conditions—no music, positive music, and negative music. Participants' moods were assessed during the experiment to confirm the mood induction manipulation. Reaction time results confirmed prior demonstrations of an interaction between a word's emotionality and its frequency. Results also showed a significant interaction between participant mood and word emotionality. However, the pattern of results was not consistent with mood-congruency effects. Although positive and negative mood facilitated responses overall in comparison to the control group, neither positive nor negative mood appeared to additionally facilitate responses to mood-congruent words. Instead, the pattern of findings seemed to be the consequence of attentional effects arising from induced mood. Positive mood broadens attention to a global level, eliminating the category distinction of positive-negative valence but leaving the high-low arousal dimension intact. In contrast, negative mood narrows attention to a local level, enhancing within-category distinctions, in particular, for negative words, resulting in less effective facilitation.","tags":["emotion","mood induction","valence","arousal","word frequency","visual word recognition","lexcial decision"],"title":"Emotion word processing: Does mood make a difference?","type":"publication"},{"authors":["Bo Yao","Christoph Scheepers"],"categories":null,"content":"","date":1435104000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435104000,"objectID":"1657ba3c8c376573a0aab7dcb20b1510","permalink":"https://boyao.science/publication/2015-ys-eaipisp/","publishdate":"2015-06-24T00:00:00Z","relpermalink":"/publication/2015-ys-eaipisp/","section":"publication","summary":"In this chapter, we review recent research concerned with “inner voice” experiences during silent reading of direct speech (e.g., _Mary said, “This dress is beautiful!”_) and indirect speech (e.g., _Mary said that the dress was beautiful_). Converging findings from speech analysis, brain imaging, and eye tracking indicate that readers spontaneously engage in mental simulations of audible-speech like representations during silent reading of direct speech, and to a much lesser extent during silent reading of indirect speech. This “simulated” implicit prosody is highly correlated with the overt prosody generated during actual speaking. We then compare this “simulated” implicit prosody with the sort of “default” implicit prosody that is commonly discussed in relation to syntactic ambiguity resolution. We hope our discussion will motivate new interdisciplinary research into prosodic processing during reading which could potentially unify the two phenomena within a single theoretical framework.","tags":["inner speech","implicit prosody","silent reading","fMRI","eye tracking"],"title":"Inner voice experiences during processing of direct and indirect speech","type":"publication"},{"authors":["Bo Yao","Graham G. Scott","Phil McAleer","Patrick J. O'Donnell","Sara C. Sereno"],"categories":null,"content":"","date":1407888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1407888000,"objectID":"8ce4ca398233a1cdecc962a3f68daed8","permalink":"https://boyao.science/publication/2014-ysmos-plosone/","publishdate":"2014-08-13T00:00:00Z","relpermalink":"/publication/2014-ysmos-plosone/","section":"publication","summary":"Although gossip serves several important social functions, it has relatively infrequently been the topic of systematic investigation. In two experiments, we advance a cognitive-informational approach to gossip. Specifically, we sought to determine which informational components engender gossip. In Experiment 1, participants read brief passages about other people and indicated their likelihood to share this information. We manipulated target familiarity (celebrity, non-celebrity) and story interest (interesting, boring). While participants were more likely to gossip about celebrity than non-celebrity targets and interesting than boring stories, they were even more likely to gossip about celebrity targets embedded within interesting stories. In Experiment 2, we additionally probed participants' reactions to the stories concerning emotion, expectation, and reputation information conveyed. Analyses showed that while such information partially mediated target familiarity and story interest effects, only expectation and reputation accounted for the interactive pattern of gossip behavior. Our findings provide novel insights into the essential components and processing mechanisms of gossip.","tags":["gossip","moderated mediation analysis","expectation","emotion"],"title":"Familiarity with interest breeds gossip: Contributions of emotion, expectation, and reputation","type":"publication"},{"authors":["Bo Yao","Milica Vasiljevic","Mario Weick","Margaret E. Sereno","Patrick J. O'Donnell","Sara C. Sereno"],"categories":null,"content":"","date":1380067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1380067200,"objectID":"f130d3013c0950a2dfe529ef15393d7c","permalink":"https://boyao.science/publication/2013-yvwsos-plosone/","publishdate":"2013-09-25T00:00:00Z","relpermalink":"/publication/2013-yvwsos-plosone/","section":"publication","summary":"Size is an important visuo-spatial characteristic of the physical world. In language processing, previous research has demonstrated a processing advantage for words denoting semantically “big” (e.g., *jungle* versus “small” (e.g., *needle*) concrete objects. We investigated whether semantic size plays a role in the recognition of words expressing abstract concepts (e.g., *truth*). Semantically “big” and “small” concrete and abstract words were presented in a lexical decision task. Responses to “big” words, regardless of their concreteness, were faster than those to “small” words. Critically, we explored the relationship between semantic size and affective characteristics of words as well as their influence on lexical access. Although a word’s semantic size was correlated with its emotional arousal, the temporal locus of arousal effects may depend on the level of concreteness. That is, arousal seemed to have an earlier (lexical) effect on abstract words, but a later (post-lexical) effect on concrete words. Our findings provide novel insights into the semantic representations of size in abstract concepts and highlight that affective attributes of words may not always index lexical access.","tags":["abstract concepts","semantic size","lexical decision","embodied cognition","emotion"],"title":"Semantic size of abstract concepts: It gets emotional when you can’t see it","type":"publication"},{"authors":["Bo Yao","Pascal Belin","Christoph Scheepers"],"categories":null,"content":"","date":1334448000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1334448000,"objectID":"6ce408e234d1952eaecb21e92063c33f","permalink":"https://boyao.science/publication/2012-ybs-neuroimage/","publishdate":"2012-04-15T00:00:00Z","relpermalink":"/publication/2012-ybs-neuroimage/","section":"publication","summary":"In human communication, direct speech (e.g., _Mary said, “I'm hungry”_) is perceived as more vivid than indirect speech (e.g., _Mary said that she was hungry_). This vividness distinction has previously been found to underlie silent reading of quotations: Using functional magnetic resonance imaging (fMRI), we found that direct speech elicited higher brain activity in the temporal voice areas (TVA) of the auditory cortex than indirect speech, consistent with an “inner voice” experience in reading direct speech. Here we show that listening to monotonously spoken direct versus indirect speech quotations also engenders differential TVA activity. This suggests that individuals engage in top-down simulations or imagery of enriched supra-segmental acoustic representations while listening to monotonous direct speech. The findings shed new light on the acoustic nature of the “inner voice” in understanding direct speech.","tags":["speech perception","fMRI","mental simulation","embodied cognition","emotional prosody"],"title":"Brain ‘talks over’ boring quotes: Top-down activation of voice-selective areas while listening to monotonous direct speech quotations","type":"publication"},{"authors":["Bo Yao","Christoph Scheepers"],"categories":null,"content":"","date":1322697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1322697600,"objectID":"f30394a245cd0d6626f2c5d74c7304b1","permalink":"https://boyao.science/publication/2011-ys-cognition/","publishdate":"2011-12-01T00:00:00Z","relpermalink":"/publication/2011-ys-cognition/","section":"publication","summary":"In human communication, direct speech (e.g., *Mary said, “I’m hungry”*) is perceived to be more vivid than indirect speech (e.g., *Mary said [that] she was hungry*). However, the processing consequences of this distinction are largely unclear. In two experiments, participants were asked to either orally (Experiment 1) or silently (Experiment 2, eye-tracking) read written stories that contained either a direct speech or an indirect speech quotation. The context preceding those quotations described a situation that implied either a fast-speaking or a slow-speaking quoted protagonist. It was found that this context manipulation affected reading rates (in both oral and silent reading) for direct speech quotations, but not for indirect speech quotations. This suggests that readers are more likely to engage in perceptual simulations of the reported speech act when reading direct speech as opposed to meaning-equivalent indirect speech quotations, as part of a more vivid representation of the former.","tags":["inner speech","silent reading","eye tracking","mental simulation","embodied cognition"],"title":"Contextual modulation of reading rate for direct versus indirect speech quotations","type":"publication"},{"authors":["Bo Yao","Pascal Belin","Christoph Scheepers"],"categories":null,"content":"","date":1317427200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1317427200,"objectID":"9e625fdf956472c99a5cb618cc5a82b5","permalink":"https://boyao.science/publication/2011-ybs-jocn/","publishdate":"2011-10-01T00:00:00Z","relpermalink":"/publication/2011-ybs-jocn/","section":"publication","summary":"In human communication, direct speech (e.g., _Mary said, “I'm hungry”_) is perceived to be more vivid than indirect speech (e.g., _Mary said [that] she was hungry_). However, for silent reading, the representational consequences of this distinction are still unclear. Although many of us share the intuition of an “inner voice,” particularly during silent reading of direct speech statements in text, there has been little direct empirical confirmation of this experience so far. Combining fMRI with eye tracking in human volunteers, we show that silent reading of direct versus indirect speech engenders differential brain activation in voice-selective areas of the auditory cortex. This suggests that readers are indeed more likely to engage in perceptual simulations (or spontaneous imagery) of the reported speaker's voice when reading direct speech as opposed to meaning-equivalent indirect speech statements as part of a more vivid representation of the former. Our results may be interpreted in line with embodied cognition and form a starting point for more sophisticated interdisciplinary research on the nature of auditory mental simulation during reading.","tags":["inner speech","silent reading","fMRI","eye tracking","mental simulation","embodied cognition"],"title":"Silent reading of direct versus indirect speech activates voice-selective areas in the auditory cortex","type":"publication"}]